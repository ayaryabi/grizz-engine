{
  "workflow_id": "workflow_1748615258",
  "created_at": "2025-05-30T16:27:38.125249",
  "status": "completed_successfully",
  "steps_completed": [
    "initialize",
    "format",
    "summarize",
    "categorize",
    "save"
  ],
  "current_step": null,
  "content": "\n    yp this essay is form paul grahm can u save it for me?\n\nApril 2003\n\n(This essay is derived from a keynote talk at PyCon 2003.)\n\nIt's hard to predict what life will be like in a hundred years. There are only a few things we can say with certainty. We know that everyone will drive flying cars, that zoning laws will be relaxed to allow buildings hundreds of stories tall, that it will be dark most of the time, and that women will all be trained in the martial arts. Here I want to zoom in on one detail of this picture. What kind of programming language will they use to write the software controlling those flying cars?\n\nThis is worth thinking about not so much because we'll actually get to use these languages as because, if we're lucky, we'll use languages on the path from this point to that.\n\nI think that, like species, languages will form evolutionary trees, with dead-ends branching off all over. We can see this happening already. Cobol, for all its sometime popularity, does not seem to have any intellectual descendants. It is an evolutionary dead-end-- a Neanderthal language.\n\nI predict a similar fate for Java. People sometimes send me mail saying, \"How can you say that Java won't turn out to be a successful language? It's already a successful language.\" And I admit that it is, if you measure success by shelf space taken up by books on it (particularly individual books on it), or by the number of undergrads who believe they have to learn it to get a job. When I say Java won't turn out to be a successful language, I mean something more specific: that Java will turn out to be an evolutionary dead-end, like Cobol.\n\nThis is just a guess. I may be wrong. My point here is not to dis Java, but to raise the issue of evolutionary trees and get people asking, where on the tree is language X? The reason to ask this question isn't just so that our ghosts can say, in a hundred years, I told you so. It's because staying close to the main branches is a useful heuristic for finding languages that will be good to program in now.\n\nAt any given time, you're probably happiest on the main branches of an evolutionary tree. Even when there were still plenty of Neanderthals, it must have sucked to be one. The Cro-Magnons would have been constantly coming over and beating you up and stealing your food.\n\nThe reason I want to know what languages will be like in a hundred years is so that I know what branch of the tree to bet on now.\n\nThe evolution of languages differs from the evolution of species because branches can converge. The Fortran branch, for example, seems to be merging with the descendants of Algol. In theory this is possible for species too, but it's not likely to have happened to any bigger than a cell.\n\nConvergence is more likely for languages partly because the space of possibilities is smaller, and partly because mutations are not random. Language designers deliberately incorporate ideas from other languages.\n\nIt's especially useful for language designers to think about where the evolution of programming languages is likely to lead, because they can steer accordingly. In that case, \"stay on a main branch\" becomes more than a way to choose a good language. It becomes a heuristic for making the right decisions about language design.\n\nAny programming language can be divided into two parts: some set of fundamental operators that play the role of axioms, and the rest of the language, which could in principle be written in terms of these fundamental operators.\n\nI think the fundamental operators are the most important factor in a language's long term survival. The rest you can change. It's like the rule that in buying a house you should consider location first of all. Everything else you can fix later, but you can't fix the location.\n\nI think it's important not just that the axioms be well chosen, but that there be few of them. Mathematicians have always felt this way about axioms-- the fewer, the better-- and I think they're onto something.\n\nAt the very least, it has to be a useful exercise to look closely at the core of a language to see if there are any axioms that could be weeded out. I've found in my long career as a slob that cruft breeds cruft, and I've seen this happen in software as well as under beds and in the corners of rooms.\n\nI have a hunch that the main branches of the evolutionary tree pass through the languages that have the smallest, cleanest cores. The more of a language you can write in itself, the better.\n\nOf course, I'm making a big assumption in even asking what programming languages will be like in a hundred years. Will we even be writing programs in a hundred years? Won't we just tell computers what we want them to do?\n\nThere hasn't been a lot of progress in that department so far. My guess is that a hundred years from now people will still tell computers what to do using programs we would recognize as such. There may be tasks that we solve now by writing programs and which in a hundred years you won't have to write programs to solve, but I think there will still be a good deal of programming of the type that we do today.\n\nIt may seem presumptuous to think anyone can predict what any technology will look like in a hundred years. But remember that we already have almost fifty years of history behind us. Looking forward a hundred years is a graspable idea when we consider how slowly languages have evolved in the past fifty.\n\nLanguages evolve slowly because they're not really technologies. Languages are notation. A program is a formal description of the problem you want a computer to solve for you. So the rate of evolution in programming languages is more like the rate of evolution in mathematical notation than, say, transportation or communications. Mathematical notation does evolve, but not with the giant leaps you see in technology.\n\nWhatever computers are made of in a hundred years, it seems safe to predict they will be much faster than they are now. If Moore's Law continues to put out, they will be 74 quintillion (73,786,976,294,838,206,464) times faster. That's kind of hard to imagine. And indeed, the most likely prediction in the speed department may be that Moore's Law will stop working. Anything that is supposed to double every eighteen months seems likely to run up against some kind of fundamental limit eventually. But I have no trouble believing that computers will be very much faster. Even if they only end up being a paltry million times faster, that should change the ground rules for programming languages substantially. Among other things, there will be more room for what would now be considered slow languages, meaning languages that don't yield very efficient code.\n\nAnd yet some applications will still demand speed. Some of the problems we want to solve with computers are created by computers; for example, the rate at which you have to process video images depends on the rate at which another computer can generate them. And there is another class of problems which inherently have an unlimited capacity to soak up cycles: image rendering, cryptography, simulations.\n\nIf some applications can be increasingly inefficient while others continue to demand all the speed the hardware can deliver, faster computers will mean that languages have to cover an ever wider range of efficiencies. We've seen this happening already. Current implementations of some popular new languages are shockingly wasteful by the standards of previous decades.\n\nThis isn't just something that happens with programming languages. It's a general historical trend. As technologies improve, each generation can do things that the previous generation would have considered wasteful. People thirty years ago would be astonished at how casually we make long distance phone calls. People a hundred years ago would be even more astonished that a package would one day travel from Boston to New York via Memphis.\n\nI can already tell you what's going to happen to all those extra cycles that faster hardware is going to give us in the next hundred years. They're nearly all going to be wasted.\n\nI learned to program when computer power was scarce. I can remember taking all the spaces out of my Basic programs so they would fit into the memory of a 4K TRS-80. The thought of all this stupendously inefficient software burning up cycles doing the same thing over and over seems kind of gross to me. But I think my intuitions here are wrong. I'm like someone who grew up poor, and can't bear to spend money even for something important, like going to the doctor.\n\nSome kinds of waste really are disgusting. SUVs, for example, would arguably be gross even if they ran on a fuel which would never run out and generated no pollution. SUVs are gross because they're the solution to a gross problem. (How to make minivans look more masculine.) But not all waste is bad. Now that we have the infrastructure to support it, counting the minutes of your long-distance calls starts to seem niggling. If you have the resources, it's more elegant to think of all phone calls as one kind of thing, no matter where the other person is.\n\nThere's good waste, and bad waste. I'm interested in good waste-- the kind where, by spending more, we can get simpler designs. How will we take advantage of the opportunities to waste cycles that we'll get from new, faster hardware?\n\nThe desire for speed is so deeply engrained in us, with our puny computers, that it will take a conscious effort to overcome it. In language design, we should be consciously seeking out situations where we can trade efficiency for even the smallest increase in convenience.\n\nMost data structures exist because of speed. For example, many languages today have both strings and lists. Semantically, strings are more or less a subset of lists in which the elements are characters. So why do you need a separate data type? You don't, really. Strings only exist for efficiency. But it's lame to clutter up the semantics of the language with hacks to make programs run faster. Having strings in a language seems to be a case of premature optimization.\n\nIf we think of the core of a language as a set of axioms, surely it's gross to have additional axioms that add no expressive power, simply for the sake of efficiency. Efficiency is important, but I don't think that's the right way to get it.\n\nThe right way to solve that problem, I think, is to separate the meaning of a program from the implementation details. Instead of having both lists and strings, have just lists, with some way to give the compiler optimization advice that will allow it to lay out strings as contiguous bytes if necessary.\n\nSince speed doesn't matter in most of a program, you won't ordinarily need to bother with this sort of micromanagement. This will be more and more true as computers get faster.\n\nSaying less about implementation should also make programs more flexible. Specifications change while a program is being written, and this is not only inevitable, but desirable.\n\nThe word \"essay\" comes from the French verb \"essayer\", which means \"to try\". An essay, in the original sense, is something you write to try to figure something out. This happens in software too. I think some of the best programs were essays, in the sense that the authors didn't know when they started exactly what they were trying to write.\n\nLisp hackers already know about the value of being flexible with data structures. We tend to write the first version of a program so that it does everything with lists. These initial versions can be so shockingly inefficient that it takes a conscious effort not to think about what they're doing, just as, for me at least, eating a steak requires a conscious effort not to think where it came from.\n\nWhat programmers in a hundred years will be looking for, most of all, is a language where you can throw together an unbelievably inefficient version 1 of a program with the least possible effort. At least, that's how we'd describe it in present-day terms. What they'll say is that they want a language that's easy to program in.\n\nInefficient software isn't gross. What's gross is a language that makes programmers do needless work. Wasting programmer time is the true inefficiency, not wasting machine time. This will become ever more clear as computers get faster.\n\nI think getting rid of strings is already something we could bear to think about. We did it in Arc, and it seems to be a win; some operations that would be awkward to describe as regular expressions can be described easily as recursive functions.\n\nHow far will this flattening of data structures go? I can think of possibilities that shock even me, with my conscientiously broadened mind. Will we get rid of arrays, for example? After all, they're just a subset of hash tables where the keys are vectors of integers. Will we replace hash tables themselves with lists?\n\nThere are more shocking prospects even than that. The Lisp that McCarthy described in 1960, for example, didn't have numbers. Logically, you don't need to have a separate notion of numbers, because you can represent them as lists: the integer n could be represented as a list of n elements. You can do math this way. It's just unbearably inefficient.\n\nNo one actually proposed implementing numbers as lists in practice. In fact, McCarthy's 1960 paper was not, at the time, intended to be implemented at all. It was a theoretical exercise, an attempt to create a more elegant alternative to the Turing Machine. When someone did, unexpectedly, take this paper and translate it into a working Lisp interpreter, numbers certainly weren't represented as lists; they were represented in binary, as in every other language.\n\nCould a programming language go so far as to get rid of numbers as a fundamental data type? I ask this not so much as a serious question as as a way to play chicken with the future. It's like the hypothetical case of an irresistible force meeting an immovable object-- here, an unimaginably inefficient implementation meeting unimaginably great resources. I don't see why not. The future is pretty long. If there's something we can do to decrease the number of axioms in the core language, that would seem to be the side to bet on as t approaches infinity. If the idea still seems unbearable in a hundred years, maybe it won't in a thousand.\n\nJust to be clear about this, I'm not proposing that all numerical calculations would actually be carried out using lists. I'm proposing that the core language, prior to any additional notations about implementation, be defined this way. In practice any program that wanted to do any amount of math would probably represent numbers in binary, but this would be an optimization, not part of the core language semantics.\n\nAnother way to burn up cycles is to have many layers of software between the application and the hardware. This too is a trend we see happening already: many recent languages are compiled into byte code. Bill Woods once told me that, as a rule of thumb, each layer of interpretation costs a factor of 10 in speed. This extra cost buys you flexibility.\n\nThe very first version of Arc was an extreme case of this sort of multi-level slowness, with corresponding benefits. It was a classic \"metacircular\" interpreter written on top of Common Lisp, with a definite family resemblance to the eval function defined in McCarthy's original Lisp paper. The whole thing was only a couple hundred lines of code, so it was very easy to understand and change. The Common Lisp we used, CLisp, itself runs on top of a byte code interpreter. So here we had two levels of interpretation, one of them (the top one) shockingly inefficient, and the language was usable. Barely usable, I admit, but usable.\n\nWriting software as multiple layers is a powerful technique even within applications. Bottom-up programming means writing a program as a series of layers, each of which serves as a language for the one above. This approach tends to yield smaller, more flexible programs. It's also the best route to that holy grail, reusability. A language is by definition reusable. The more of your application you can push down into a language for writing that type of application, the more of your software will be reusable.\n\nSomehow the idea of reusability got attached to object-oriented programming in the 1980s, and no amount of evidence to the contrary seems to be able to shake it free. But although some object-oriented software is reusable, what makes it reusable is its bottom-upness, not its object-orientedness. Consider libraries: they're reusable because they're language, whether they're written in an object-oriented style or not.\n\nI don't predict the demise of object-oriented programming, by the way. Though I don't think it has much to offer good programmers, except in certain specialized domains, it is irresistible to large organizations. Object-oriented programming offers a sustainable way to write spaghetti code. It lets you accrete programs as a series of patches. Large organizations always tend to develop software this way, and I expect this to be as true in a hundred years as it is today.\n\nAs long as we're talking about the future, we had better talk about parallel computation, because that's where this idea seems to live. That is, no matter when you're talking, parallel computation seems to be something that is going to happen in the future.\n\nWill the future ever catch up with it? People have been talking about parallel computation as something imminent for at least 20 years, and it hasn't affected programming practice much so far. Or hasn't it? Already chip designers have to think about it, and so must people trying to write systems software on multi-cpu computers.\n\nThe real question is, how far up the ladder of abstraction will parallelism go? In a hundred years will it affect even application programmers? Or will it be something that compiler writers think about, but which is usually invisible in the source code of applications?\n\nOne thing that does seem likely is that most opportunities for parallelism will be wasted. This is a special case of my more general prediction that most of the extra computer power we're given will go to waste. I expect that, as with the stupendous speed of the underlying hardware, parallelism will be something that is available if you ask for it explicitly, but ordinarily not used. This implies that the kind of parallelism we have in a hundred years will not, except in special applications, be massive parallelism. I expect for ordinary programmers it will be more like being able to fork off processes that all end up running in parallel.\n\nAnd this will, like asking for specific implementations of data structures, be something that you do fairly late in the life of a program, when you try to optimize it. Version 1s will ordinarily ignore any advantages to be got from parallel computation, just as they will ignore advantages to be got from specific representations of data.\n\nExcept in special kinds of applications, parallelism won't pervade the programs that are written in a hundred years. It would be premature optimization if it did.\n\nHow many programming languages will there be in a hundred years? There seem to be a huge number of new programming languages lately. Part of the reason is that faster hardware has allowed programmers to make different tradeoffs between speed and convenience, depending on the application. If this is a real trend, the hardware we'll have in a hundred years should only increase it.\n\nAnd yet there may be only a few widely-used languages in a hundred years. Part of the reason I say this is optimism: it seems that, if you did a really good job, you could make a language that was ideal for writing a slow version 1, and yet with the right optimization advice to the compiler, would also yield very fast code when necessary. So, since I'm optimistic, I'm going to predict that despite the huge gap they'll have between acceptable and maximal efficiency, programmers in a hundred years will have languages that can span most of it.\n\nAs this gap widens, profilers will become increasingly important. Little attention is paid to profiling now. Many people still seem to believe that the way to get fast applications is to write compilers that generate fast code. As the gap between acceptable and maximal performance widens, it will become increasingly clear that the way to get fast applications is to have a good guide from one to the other.\n\nWhen I say there may only be a few languages, I'm not including domain-specific \"little languages\". I think such embedded languages are a great idea, and I expect them to proliferate. But I expect them to be written as thin enough skins that users can see the general-purpose language underneath.\n\nWho will design the languages of the future? One of the most exciting trends in the last ten years has been the rise of open-source languages like Perl, Python, and Ruby. Language design is being taken over by hackers. The results so far are messy, but encouraging. There are some stunningly novel ideas in Perl, for example. Many are stunningly bad, but that's always true of ambitious efforts. At its current rate of mutation, God knows what Perl might evolve into in a hundred years.\n\nIt's not true that those who can't do, teach (some of the best hackers I know are professors), but it is true that there are a lot of things that those who teach can't do. Research imposes constraining caste restrictions. In any academic field there are topics that are ok to work on and others that aren't. Unfortunately the distinction between acceptable and forbidden topics is usually based on how intellectual the work sounds when described in research papers, rather than how important it is for getting good results. The extreme case is probably literature; people studying literature rarely say anything that would be of the slightest use to those producing it.\n\nThough the situation is better in the sciences, the overlap between the kind of work you're allowed to do and the kind of work that yields good languages is distressingly small. (Olin Shivers has grumbled eloquently about this.) For example, types seem to be an inexhaustible source of research papers, despite the fact that static typing seems to preclude true macros-- without which, in my opinion, no language is worth using.\n\nThe trend is not merely toward languages being developed as open-source projects rather than \"research\", but toward languages being designed by the application programmers who need to use them, rather than by compiler writers. This seems a good trend and I expect it to continue.\n\nUnlike physics in a hundred years, which is almost necessarily impossible to predict, I think it may be possible in principle to design a language now that would appeal to users in a hundred years.\n\nOne way to design a language is to just write down the program you'd like to be able to write, regardless of whether there is a compiler that can translate it or hardware that can run it. When you do this you can assume unlimited resources. It seems like we ought to be able to imagine unlimited resources as well today as in a hundred years.\n\nWhat program would one like to write? Whatever is least work. Except not quite: whatever would be least work if your ideas about programming weren't already influenced by the languages you're currently used to. Such influence can be so pervasive that it takes a great effort to overcome it. You'd think it would be obvious to creatures as lazy as us how to express a program with the least effort. In fact, our ideas about what's possible tend to be so limited by whatever language we think in that easier formulations of programs seem very surprising. They're something you have to discover, not something you naturally sink into.\n\nOne helpful trick here is to use the length of the program as an approximation for how much work it is to write. Not the length in characters, of course, but the length in distinct syntactic elements-- basically, the size of the parse tree. It may not be quite true that the shortest program is the least work to write, but it's close enough that you're better off aiming for the solid target of brevity than the fuzzy, nearby one of least work. Then the algorithm for language design becomes: look at a program and ask, is there any way to write this that's shorter?\n\nIn practice, writing programs in an imaginary hundred-year language will work to varying degrees depending on how close you are to the core. Sort routines you can write now. But it would be hard to predict now what kinds of libraries might be needed in a hundred years. Presumably many libraries will be for domains that don't even exist yet. If SETI@home works, for example, we'll need libraries for communicating with aliens. Unless of course they are sufficiently advanced that they already communicate in XML.\n\nAt the other extreme, I think you might be able to design the core language today. In fact, some might argue that it was already mostly designed in 1958.\n\nIf the hundred year language were available today, would we want to program in it? One way to answer this question is to look back. If present-day programming languages had been available in 1960, would anyone have wanted to use them?\n\nIn some ways, the answer is no. Languages today assume infrastructure that didn't exist in 1960. For example, a language in which indentation is significant, like Python, would not work very well on printer terminals. But putting such problems aside-- assuming, for example, that programs were all just written on paper-- would programmers of the 1960s have liked writing programs in the languages we use now?\n\nI think so. Some of the less imaginative ones, who had artifacts of early languages built into their ideas of what a program was, might have had trouble. (How can you manipulate data without doing pointer arithmetic? How can you implement flow charts without gotos?) But I think the smartest programmers would have had no trouble making the most of present-day languages, if they'd had them.\n\nIf we had the hundred-year language now, it would at least make a great pseudocode. What about using it to write software? Since the hundred-year language will need to generate fast code for some applications, presumably it could generate code efficient enough to run acceptably well on our hardware. We might have to give more optimization advice than users in a hundred years, but it still might be a net win.\n\nNow we have two ideas that, if you combine them, suggest interesting possibilities: (1) the hundred-year language could, in principle, be designed today, and (2) such a language, if it existed, might be good to program in today. When you see these ideas laid out like that, it's hard not to think, why not try writing the hundred-year language now?\n\nWhen you're working on language design, I think it is good to have such a target and to keep it consciously in mind. When you learn to drive, one of the principles they teach you is to align the car not by lining up the hood with the stripes painted on the road, but by aiming at some point in the distance. Even if all you care about is what happens in the next ten feet, this is the right answer. I think we can and should do the same thing with programming languages.\n\nNotes\n\nI believe Lisp Machine Lisp was the first language to embody the principle that declarations (except those of dynamic variables) were merely optimization advice, and would not change the meaning of a correct program. Common Lisp seems to have been the first to state this explicitly.\n\nThanks to Trevor Blackwell, Robert Morris, and Dan Giffin for reading drafts of this, and to Guido van Rossum, Jeremy Hylton, and the rest of the Python crew for inviting me to speak at PyCon.\n\nyp this essay is form paul grahm can u save it for me?\n\nApril 2003\n\n(This essay is derived from a keynote talk at PyCon 2003.)\n\nIt's hard to predict what life will be like in a hundred years. There are only a few things we can say with certainty. We know that everyone will drive flying cars, that zoning laws will be relaxed to allow buildings hundreds of stories tall, that it will be dark most of the time, and that women will all be trained in the martial arts. Here I want to zoom in on one detail of this picture. What kind of programming language will they use to write the software controlling those flying cars?\n\nThis is worth thinking about not so much because we'll actually get to use these languages as because, if we're lucky, we'll use languages on the path from this point to that.\n\nI think that, like species, languages will form evolutionary trees, with dead-ends branching off all over. We can see this happening already. Cobol, for all its sometime popularity, does not seem to have any intellectual descendants. It is an evolutionary dead-end-- a Neanderthal language.\n\nI predict a similar fate for Java. People sometimes send me mail saying, \"How can you say that Java won't turn out to be a successful language? It's already a successful language.\" And I admit that it is, if you measure success by shelf space taken up by books on it (particularly individual books on it), or by the number of undergrads who believe they have to learn it to get a job. When I say Java won't turn out to be a successful language, I mean something more specific: that Java will turn out to be an evolutionary dead-end, like Cobol.\n\nThis is just a guess. I may be wrong. My point here is not to dis Java, but to raise the issue of evolutionary trees and get people asking, where on the tree is language X? The reason to ask this question isn't just so that our ghosts can say, in a hundred years, I told you so. It's because staying close to the main branches is a useful heuristic for finding languages that will be good to program in now.\n\nAt any given time, you're probably happiest on the main branches of an evolutionary tree. Even when there were still plenty of Neanderthals, it must have sucked to be one. The Cro-Magnons would have been constantly coming over and beating you up and stealing your food.\n\nThe reason I want to know what languages will be like in a hundred years is so that I know what branch of the tree to bet on now.\n\nThe evolution of languages differs from the evolution of species because branches can converge. The Fortran branch, for example, seems to be merging with the descendants of Algol. In theory this is possible for species too, but it's not likely to have happened to any bigger than a cell.\n\nConvergence is more likely for languages partly because the space of possibilities is smaller, and partly because mutations are not random. Language designers deliberately incorporate ideas from other languages.\n\nIt's especially useful for language designers to think about where the evolution of programming languages is likely to lead, because they can steer accordingly. In that case, \"stay on a main branch\" becomes more than a way to choose a good language. It becomes a heuristic for making the right decisions about language design.\n\nAny programming language can be divided into two parts: some set of fundamental operators that play the role of axioms, and the rest of the language, which could in principle be written in terms of these fundamental operators.\n\nI think the fundamental operators are the most important factor in a language's long term survival. The rest you can change. It's like the rule that in buying a house you should consider location first of all. Everything else you can fix later, but you can't fix the location.\n\nI think it's important not just that the axioms be well chosen, but that there be few of them. Mathematicians have always felt this way about axioms-- the fewer, the better-- and I think they're onto something.\n\nAt the very least, it has to be a useful exercise to look closely at the core of a language to see if there are any axioms that could be weeded out. I've found in my long career as a slob that cruft breeds cruft, and I've seen this happen in software as well as under beds and in the corners of rooms.\n\nI have a hunch that the main branches of the evolutionary tree pass through the languages that have the smallest, cleanest cores. The more of a language you can write in itself, the better.\n\nOf course, I'm making a big assumption in even asking what programming languages will be like in a hundred years. Will we even be writing programs in a hundred years? Won't we just tell computers what we want them to do?\n\nThere hasn't been a lot of progress in that department so far. My guess is that a hundred years from now people will still tell computers what to do using programs we would recognize as such. There may be tasks that we solve now by writing programs and which in a hundred years you won't have to write programs to solve, but I think there will still be a good deal of programming of the type that we do today.\n\nIt may seem presumptuous to think anyone can predict what any technology will look like in a hundred years. But remember that we already have almost fifty years of history behind us. Looking forward a hundred years is a graspable idea when we consider how slowly languages have evolved in the past fifty.\n\nLanguages evolve slowly because they're not really technologies. Languages are notation. A program is a formal description of the problem you want a computer to solve for you. So the rate of evolution in programming languages is more like the rate of evolution in mathematical notation than, say, transportation or communications. Mathematical notation does evolve, but not with the giant leaps you see in technology.\n\nWhatever computers are made of in a hundred years, it seems safe to predict they will be much faster than they are now. If Moore's Law continues to put out, they will be 74 quintillion (73,786,976,294,838,206,464) times faster. That's kind of hard to imagine. And indeed, the most likely prediction in the speed department may be that Moore's Law will stop working. Anything that is supposed to double every eighteen months seems likely to run up against some kind of fundamental limit eventually. But I have no trouble believing that computers will be very much faster. Even if they only end up being a paltry million times faster, that should change the ground rules for programming languages substantially. Among other things, there will be more room for what would now be considered slow languages, meaning languages that don't yield very efficient code.\n\nAnd yet some applications will still demand speed. Some of the problems we want to solve with computers are created by computers; for example, the rate at which you have to process video images depends on the rate at which another computer can generate them. And there is another class of problems which inherently have an unlimited capacity to soak up cycles: image rendering, cryptography, simulations.\n\nIf some applications can be increasingly inefficient while others continue to demand all the speed the hardware can deliver, faster computers will mean that languages have to cover an ever wider range of efficiencies. We've seen this happening already. Current implementations of some popular new languages are shockingly wasteful by the standards of previous decades.\n\nThis isn't just something that happens with programming languages. It's a general historical trend. As technologies improve, each generation can do things that the previous generation would have considered wasteful. People thirty years ago would be astonished at how casually we make long distance phone calls. People a hundred years ago would be even more astonished that a package would one day travel from Boston to New York via Memphis.\n\nI can already tell you what's going to happen to all those extra cycles that faster hardware is going to give us in the next hundred years. They're nearly all going to be wasted.\n\nI learned to program when computer power was scarce. I can remember taking all the spaces out of my Basic programs so they would fit into the memory of a 4K TRS-80. The thought of all this stupendously inefficient software burning up cycles doing the same thing over and over seems kind of gross to me. But I think my intuitions here are wrong. I'm like someone who grew up poor, and can't bear to spend money even for something important, like going to the doctor.\n\nSome kinds of waste really are disgusting. SUVs, for example, would arguably be gross even if they ran on a fuel which would never run out and generated no pollution. SUVs are gross because they're the solution to a gross problem. (How to make minivans look more masculine.) But not all waste is bad. Now that we have the infrastructure to support it, counting the minutes of your long-distance calls starts to seem niggling. If you have the resources, it's more elegant to think of all phone calls as one kind of thing, no matter where the other person is.\n\nThere's good waste, and bad waste. I'm interested in good waste-- the kind where, by spending more, we can get simpler designs. How will we take advantage of the opportunities to waste cycles that we'll get from new, faster hardware?\n\nThe desire for speed is so deeply engrained in us, with our puny computers, that it will take a conscious effort to overcome it. In language design, we should be consciously seeking out situations where we can trade efficiency for even the smallest increase in convenience.\n\nMost data structures exist because of speed. For example, many languages today have both strings and lists. Semantically, strings are more or less a subset of lists in which the elements are characters. So why do you need a separate data type? You don't, really. Strings only exist for efficiency. But it's lame to clutter up the semantics of the language with hacks to make programs run faster. Having strings in a language seems to be a case of premature optimization.\n\nIf we think of the core of a language as a set of axioms, surely it's gross to have additional axioms that add no expressive power, simply for the sake of efficiency. Efficiency is important, but I don't think that's the right way to get it.\n\nThe right way to solve that problem, I think, is to separate the meaning of a program from the implementation details. Instead of having both lists and strings, have just lists, with some way to give the compiler optimization advice that will allow it to lay out strings as contiguous bytes if necessary.\n\nSince speed doesn't matter in most of a program, you won't ordinarily need to bother with this sort of micromanagement. This will be more and more true as computers get faster.\n\nSaying less about implementation should also make programs more flexible. Specifications change while a program is being written, and this is not only inevitable, but desirable.\n\nThe word \"essay\" comes from the French verb \"essayer\", which means \"to try\". An essay, in the original sense, is something you write to try to figure something out. This happens in software too. I think some of the best programs were essays, in the sense that the authors didn't know when they started exactly what they were trying to write.\n\nLisp hackers already know about the value of being flexible with data structures. We tend to write the first version of a program so that it does everything with lists. These initial versions can be so shockingly inefficient that it takes a conscious effort not to think about what they're doing, just as, for me at least, eating a steak requires a conscious effort not to think where it came from.\n\nWhat programmers in a hundred years will be looking for, most of all, is a language where you can throw together an unbelievably inefficient version 1 of a program with the least possible effort. At least, that's how we'd describe it in present-day terms. What they'll say is that they want a language that's easy to program in.\n\nInefficient software isn't gross. What's gross is a language that makes programmers do needless work. Wasting programmer time is the true inefficiency, not wasting machine time. This will become ever more clear as computers get faster.\n\nI think getting rid of strings is already something we could bear to think about. We did it in Arc, and it seems to be a win; some operations that would be awkward to describe as regular expressions can be described easily as recursive functions.\n\nHow far will this flattening of data structures go? I can think of possibilities that shock even me, with my conscientiously broadened mind. Will we get rid of arrays, for example? After all, they're just a subset of hash tables where the keys are vectors of integers. Will we replace hash tables themselves with lists?\n\nThere are more shocking prospects even than that. The Lisp that McCarthy described in 1960, for example, didn't have numbers. Logically, you don't need to have a separate notion of numbers, because you can represent them as lists: the integer n could be represented as a list of n elements. You can do math this way. It's just unbearably inefficient.\n\nNo one actually proposed implementing numbers as lists in practice. In fact, McCarthy's 1960 paper was not, at the time, intended to be implemented at all. It was a theoretical exercise, an attempt to create a more elegant alternative to the Turing Machine. When someone did, unexpectedly, take this paper and translate it into a working Lisp interpreter, numbers certainly weren't represented as lists; they were represented in binary, as in every other language.\n\nCould a programming language go so far as to get rid of numbers as a fundamental data type? I ask this not so much as a serious question as as a way to play chicken with the future. It's like the hypothetical case of an irresistible force meeting an immovable object-- here, an unimaginably inefficient implementation meeting unimaginably great resources. I don't see why not. The future is pretty long. If there's something we can do to decrease the number of axioms in the core language, that would seem to be the side to bet on as t approaches infinity. If the idea still seems unbearable in a hundred years, maybe it won't in a thousand.\n\nJust to be clear about this, I'm not proposing that all numerical calculations would actually be carried out using lists. I'm proposing that the core language, prior to any additional notations about implementation, be defined this way. In practice any program that wanted to do any amount of math would probably represent numbers in binary, but this would be an optimization, not part of the core language semantics.\n\nAnother way to burn up cycles is to have many layers of software between the application and the hardware. This too is a trend we see happening already: many recent languages are compiled into byte code. Bill Woods once told me that, as a rule of thumb, each layer of interpretation costs a factor of 10 in speed. This extra cost buys you flexibility.\n\nThe very first version of Arc was an extreme case of this sort of multi-level slowness, with corresponding benefits. It was a classic \"metacircular\" interpreter written on top of Common Lisp, with a definite family resemblance to the eval function defined in McCarthy's original Lisp paper. The whole thing was only a couple hundred lines of code, so it was very easy to understand and change. The Common Lisp we used, CLisp, itself runs on top of a byte code interpreter. So here we had two levels of interpretation, one of them (the top one) shockingly inefficient, and the language was usable. Barely usable, I admit, but usable.\n\nWriting software as multiple layers is a powerful technique even within applications. Bottom-up programming means writing a program as a series of layers, each of which serves as a language for the one above. This approach tends to yield smaller, more flexible programs. It's also the best route to that holy grail, reusability. A language is by definition reusable. The more of your application you can push down into a language for writing that type of application, the more of your software will be reusable.\n\nSomehow the idea of reusability got attached to object-oriented programming in the 1980s, and no amount of evidence to the contrary seems to be able to shake it free. But although some object-oriented software is reusable, what makes it reusable is its bottom-upness, not its object-orientedness. Consider libraries: they're reusable because they're language, whether they're written in an object-oriented style or not.\n\nI don't predict the demise of object-oriented programming, by the way. Though I don't think it has much to offer good programmers, except in certain specialized domains, it is irresistible to large organizations. Object-oriented programming offers a sustainable way to write spaghetti code. It lets you accrete programs as a series of patches. Large organizations always tend to develop software this way, and I expect this to be as true in a hundred years as it is today.\n\nAs long as we're talking about the future, we had better talk about parallel computation, because that's where this idea seems to live. That is, no matter when you're talking, parallel computation seems to be something that is going to happen in the future.\n\nWill the future ever catch up with it? People have been talking about parallel computation as something imminent for at least 20 years, and it hasn't affected programming practice much so far. Or hasn't it? Already chip designers have to think about it, and so must people trying to write systems software on multi-cpu computers.\n\nThe real question is, how far up the ladder of abstraction will parallelism go? In a hundred years will it affect even application programmers? Or will it be something that compiler writers think about, but which is usually invisible in the source code of applications?\n\nOne thing that does seem likely is that most opportunities for parallelism will be wasted. This is a special case of my more general prediction that most of the extra computer power we're given will go to waste. I expect that, as with the stupendous speed of the underlying hardware, parallelism will be something that is available if you ask for it explicitly, but ordinarily not used. This implies that the kind of parallelism we have in a hundred years will not, except in special applications, be massive parallelism. I expect for ordinary programmers it will be more like being able to fork off processes that all end up running in parallel.\n\nAnd this will, like asking for specific implementations of data structures, be something that you do fairly late in the life of a program, when you try to optimize it. Version 1s will ordinarily ignore any advantages to be got from parallel computation, just as they will ignore advantages to be got from specific representations of data.\n\nExcept in special kinds of applications, parallelism won't pervade the programs that are written in a hundred years. It would be premature optimization if it did.\n\nHow many programming languages will there be in a hundred years? There seem to be a huge number of new programming languages lately. Part of the reason is that faster hardware has allowed programmers to make different tradeoffs between speed and convenience, depending on the application. If this is a real trend, the hardware we'll have in a hundred years should only increase it.\n\nAnd yet there may be only a few widely-used languages in a hundred years. Part of the reason I say this is optimism: it seems that, if you did a really good job, you could make a language that was ideal for writing a slow version 1, and yet with the right optimization advice to the compiler, would also yield very fast code when necessary. So, since I'm optimistic, I'm going to predict that despite the huge gap they'll have between acceptable and maximal efficiency, programmers in a hundred years will have languages that can span most of it.\n\nAs this gap widens, profilers will become increasingly important. Little attention is paid to profiling now. Many people still seem to believe that the way to get fast applications is to write compilers that generate fast code. As the gap between acceptable and maximal performance widens, it will become increasingly clear that the way to get fast applications is to have a good guide from one to the other.\n\nWhen I say there may only be a few languages, I'm not including domain-specific \"little languages\". I think such embedded languages are a great idea, and I expect them to proliferate. But I expect them to be written as thin enough skins that users can see the general-purpose language underneath.\n\nWho will design the languages of the future? One of the most exciting trends in the last ten years has been the rise of open-source languages like Perl, Python, and Ruby. Language design is being taken over by hackers. The results so far are messy, but encouraging. There are some stunningly novel ideas in Perl, for example. Many are stunningly bad, but that's always true of ambitious efforts. At its current rate of mutation, God knows what Perl might evolve into in a hundred years.\n\nIt's not true that those who can't do, teach (some of the best hackers I know are professors), but it is true that there are a lot of things that those who teach can't do. Research imposes constraining caste restrictions. In any academic field there are topics that are ok to work on and others that aren't. Unfortunately the distinction between acceptable and forbidden topics is usually based on how intellectual the work sounds when described in research papers, rather than how important it is for getting good results. The extreme case is probably literature; people studying literature rarely say anything that would be of the slightest use to those producing it.\n\nThough the situation is better in the sciences, the overlap between the kind of work you're allowed to do and the kind of work that yields good languages is distressingly small. (Olin Shivers has grumbled eloquently about this.) For example, types seem to be an inexhaustible source of research papers, despite the fact that static typing seems to preclude true macros-- without which, in my opinion, no language is worth using.\n\nThe trend is not merely toward languages being developed as open-source projects rather than \"research\", but toward languages being designed by the application programmers who need to use them, rather than by compiler writers. This seems a good trend and I expect it to continue.\n\nUnlike physics in a hundred years, which is almost necessarily impossible to predict, I think it may be possible in principle to design a language now that would appeal to users in a hundred years.\n\nOne way to design a language is to just write down the program you'd like to be able to write, regardless of whether there is a compiler that can translate it or hardware that can run it. When you do this you can assume unlimited resources. It seems like we ought to be able to imagine unlimited resources as well today as in a hundred years.\n\nWhat program would one like to write? Whatever is least work. Except not quite: whatever would be least work if your ideas about programming weren't already influenced by the languages you're currently used to. Such influence can be so pervasive that it takes a great effort to overcome it. You'd think it would be obvious to creatures as lazy as us how to express a program with the least effort. In fact, our ideas about what's possible tend to be so limited by whatever language we think in that easier formulations of programs seem very surprising. They're something you have to discover, not something you naturally sink into.\n\nOne helpful trick here is to use the length of the program as an approximation for how much work it is to write. Not the length in characters, of course, but the length in distinct syntactic elements-- basically, the size of the parse tree. It may not be quite true that the shortest program is the least work to write, but it's close enough that you're better off aiming for the solid target of brevity than the fuzzy, nearby one of least work. Then the algorithm for language design becomes: look at a program and ask, is there any way to write this that's shorter?\n\nIn practice, writing programs in an imaginary hundred-year language will work to varying degrees depending on how close you are to the core. Sort routines you can write now. But it would be hard to predict now what kinds of libraries might be needed in a hundred years. Presumably many libraries will be for domains that don't even exist yet. If SETI@home works, for example, we'll need libraries for communicating with aliens. Unless of course they are sufficiently advanced that they already communicate in XML.\n\nAt the other extreme, I think you might be able to design the core language today. In fact, some might argue that it was already mostly designed in 1958.\n\nIf the hundred year language were available today, would we want to program in it? One way to answer this question is to look back. If present-day programming languages had been available in 1960, would anyone have wanted to use them?\n\nIn some ways, the answer is no. Languages today assume infrastructure that didn't exist in 1960. For example, a language in which indentation is significant, like Python, would not work very well on printer terminals. But putting such problems aside-- assuming, for example, that programs were all just written on paper-- would programmers of the 1960s have liked writing programs in the languages we use now?\n\nI think so. Some of the less imaginative ones, who had artifacts of early languages built into their ideas of what a program was, might have had trouble. (How can you manipulate data without doing pointer arithmetic? How can you implement flow charts without gotos?) But I think the smartest programmers would have had no trouble making the most of present-day languages, if they'd had them.\n\nIf we had the hundred-year language now, it would at least make a great pseudocode. What about using it to write software? Since the hundred-year language will need to generate fast code for some applications, presumably it could generate code efficient enough to run acceptably well on our hardware. We might have to give more optimization advice than users in a hundred years, but it still might be a net win.\n\nNow we have two ideas that, if you combine them, suggest interesting possibilities: (1) the hundred-year language could, in principle, be designed today, and (2) such a language, if it existed, might be good to program in today. When you see these ideas laid out like that, it's hard not to think, why not try writing the hundred-year language now?\n\nWhen you're working on language design, I think it is good to have such a target and to keep it consciously in mind. When you learn to drive, one of the principles they teach you is to align the car not by lining up the hood with the stripes painted on the road, but by aiming at some point in the distance. Even if all you care about is what happens in the next ten feet, this is the right answer. I think we can and should do the same thing with programming languages.\n\nNotes\n\nI believe Lisp Machine Lisp was the first language to embody the principle that declarations (except those of dynamic variables) were merely optimization advice, and would not change the meaning of a correct program. Common Lisp seems to have been the first to state this explicitly.\n\nThanks to Trevor Blackwell, Robert Morris, and Dan Giffin for reading drafts of this, and to Guido van Rossum, Jeremy Hylton, and the rest of the Python crew for inviting me to speak at PyCon.\n    ",
  "formatted_content": "# What Programming Will Be Like in a Hundred Years  \n*Paul Graham, April 2003*  \n*(Derived from a keynote talk at PyCon 2003)*  \n\n---\n\n## Introduction  \nIt is difficult to predict life in a hundred years. Some certainties might include: flying cars will be common, zoning laws will allow very tall buildings, it will be dark most of the time, and women will be trained in martial arts. This essay focuses on one detail:  \n\n**What kind of programming language will people use to write the software controlling those flying cars?**  \n\nUnderstanding this is valuable not necessarily because we will directly use such languages, but because we may use languages that lead up to them.  \n\n---\n\n## Languages as Evolutionary Trees  \n- Programming languages evolve like species, forming evolutionary trees with many dead ends.  \n- Example: COBOL is an evolutionary dead end, like Neanderthals.  \n- Prediction: Java will have a similar fate, despite its current popularity and success in education and books.  \n\n### Key Insight  \n\"Staying close to the main branches\" of the evolutionary tree is a useful heuristic for finding languages good to program in now.  \n\n---\n\n## Characteristics of Evolution in Programming Languages  \n- Unlike biological species, language branches can converge (e.g., Fortran and Algol descendants).  \n- This convergence is possible because:  \n  - The space of possibilities for languages is smaller.  \n  - Mutations in languages are deliberate, often inspired by others.  \n- Language designers can steer evolution and should aim to stay on the main branches.\n\n---\n\n## Core Language Design Principles  \n- Every language can be divided into:  \n  - Fundamental operators (axioms).  \n  - Other parts that can be written in terms of these fundamental operators.  \n\n### Important Points About Axioms  \n- The choice and the number of axioms are critical.  \n- Fewer axioms are better (akin to mathematical elegance).  \n- Cruft (unnecessary elements) breeds more cruft; smaller, cleaner cores are more likely to survive evolution.  \n\n---\n\n## Will We Be Writing Programs in 100 Years?  \n- Despite hopes for natural language interfaces, programming in the traditional sense will likely persist.  \n- Programs are formal descriptions, akin to mathematical notation, which evolves slowly.  \n- Computer hardware will be exponentially faster due to Moore's Law but may reach physical limits.  \n- Increased hardware speed allows more inefficient languages but leaves room for applications that still require speed-sensitive code (e.g., video processing, cryptography).  \n\n---\n\n## Efficiency and Wasting Cycles  \n- As hardware improves, many new applications will allow inefficient software that sacrifices speed for convenience.  \n- Distinguish between:  \n  - **Bad waste** (like SUVs, solutions to unnecessary problems).  \n  - **Good waste** (wasting resources to produce simpler, more elegant designs).  \n- In language design, trading efficiency for convenience should be consciously sought.  \n\n---\n\n## Data Structures and Premature Optimization  \n- Having separate types like strings and lists is primarily for efficiency.  \n- It's better to have a simpler core (e.g., only lists) and use compiler hints for optimizations rather than clunky separate types.  \n- Flexibility in implementation details leads to more adaptable programs.  \n\n---\n\n## Flexibility and Programming Ease  \n- Many good programs begin as \"essays\"\u2014not fully understood at the start.  \n- Early versions tend to be inefficient but easy to write.  \n- The future language will favor ease of programming even if it means inefficient implementations at first.  \n- Programmer time is more valuable than machine time, especially as computers get faster.\n\n---\n\n## Flattening of Data Structures\u2014How Far?  \n- Possible future simplifications:  \n  - Getting rid of arrays (viewed as subsets of hash tables).  \n  - Replacing hash tables with lists.  \n- Even numbers could theoretically be represented as lists, though inefficiently (inspired by McCarthy\u2019s 1960 Lisp).  \n- The core language might be minimal, with efficiencies handled by optimizations.\n\n---\n\n## Layers of Software and Interpretation  \n- Multi-layered interpretation is common (e.g., recent languages compiled to bytecode, interpreted multiple times).  \n- Each interpretation layer typically costs about a factor of 10 in speed but gains flexibility.  \n- Example:  \n  - Arc language is a metacircular interpreter on Common Lisp, which runs on a bytecode interpreter.  \n- Writing software in layers enables modularity and reusability.  \n\n---\n\n## Object-Oriented Programming and Reusability  \n- Reusability is a result of \"bottom-up\" programming, not just object orientation.  \n- Libraries are reusable because they provide languages for specific tasks, regardless of OOP style.  \n- OOP persists due to large organizations using it to manage sprawling, patchy codebases.  \n\n---\n\n## Parallel Computation  \n- Parallelism remains perpetually a \"future\" technology but is slowly becoming a necessity.  \n- Questions:  \n  - Will parallelism be exposed at application level in 100 years?  \n  - Or will it remain a compiler/system-level concern?  \n- Likely scenario: parallelism often wasted; explicit but limited use for optimization, not pervasive.  \n- Premature optimization by parallelism should be avoided.\n\n---\n\n## Number of Programming Languages in 100 Years  \n- Recent explosion of languages due to differing trade-offs enabled by faster hardware.  \n- Future hardware will likely increase this trend.  \n- However, possibly only a few widely-used general-purpose languages will persist, spanning a broad spectrum of efficiencies.  \n- Domain-specific embedded languages (\"little languages\") will proliferate as thin layers over general-purpose languages.  \n\n---\n\n## Who Will Design Future Languages?  \n- Open-source hackers increasingly lead language design (Perl, Python, Ruby).  \n- Academic research can be disconnected from practical language design.  \n- Application programmers creating languages to meet their needs is a major trend.  \n\n---\n\n## Designing a Language for the Next Hundred Years  \n- Design from the ideal program, assuming unlimited resources and ignoring current language biases.  \n- Shorter programs (by parse tree size) roughly correlate with less effort to write and better language design.  \n- Libraries for future domains (like alien communication) may appear, but core language design can be mostly done today (some argue it was done by 1958).  \n\n---\n\n## Retrospective Test  \n- Would programmers in the 1960s have liked today\u2019s languages?  \n- Many would, assuming infrastructure issues (e.g., printer terminals) aside.  \n- Today\u2019s languages would have made good pseudocode and may be usable now despite optimization needs.  \n\n---\n\n## Conclusion: Aim for the Distant Target  \n- Combine two ideas:  \n  1. The hundred-year language could be designed today.  \n  2. Such a language might be beneficial to use now.  \n\n- Language designers should keep this distant but clear target in mind, like aiming a car at a distant point on the road\u2014not just what is visible immediately.  \n\n---\n\n## Notes  \n- Lisp Machine Lisp pioneered the principle that declarations are optimization advice and do not change program meaning; Common Lisp was first to state this explicitly.  \n- Thanks to Trevor Blackwell, Robert Morris, Dan Giffin, Guido van Rossum, Jeremy Hylton, and the Python community.\n\n---\n\n*Essay originally by Paul Graham, April 2003*",
  "summary": "```markdown\n# Summary of \"What Programming Will Be Like in a Hundred Years\" by Paul Graham (2003)\n\n## Key Insights\n\n- **Languages evolve like species**, forming evolutionary trees with many branches and dead ends (e.g., COBOL, likely Java).\n- Staying close to the \"main branches\" of language evolution is a good heuristic for choosing programming languages today.\n\n- Unlike biological evolution, programming languages can **converge** due to smaller possibility space and deliberate mutation inspired by other languages.\n\n- **Core language design depends on a small set of fundamental operators (axioms).**  \n  - Fewer axioms mean greater elegance and less \"cruft.\"  \n  - Smaller, cleaner cores are more likely to survive evolution.\n\n- **Programming will continue to exist in 100 years.**  \n  - Natural language interfaces won\u2019t fully replace formal programming, as programs are akin to mathematical notation that changes slowly.  \n  - Hardware speed improvements allow more inefficient languages but do not eliminate the need for efficiency in some domains (e.g., video processing, cryptography).\n\n- **Trade-offs between efficiency and convenience:**  \n  - \"Good waste\" trades some efficiency for simpler, more elegant designs.  \n  - Future languages will favor programmer time over machine time, given faster hardware.\n\n- **Data structures will simplify:**  \n  - The future may flatten distinctions (arrays as hash tables, numbers as lists).  \n  - Efficiency will be handled mostly by compiler optimizations rather than rigid type distinctions.\n\n- **Software will be built in multi-layered interpretations** providing modularity and reusability despite performance costs.\n\n- **Object-Oriented Programming (OOP)** is mainly useful for managing large, complex codebases but reusability comes from \"bottom-up\" programming and building reusable languages (libraries).\n\n- **Parallel computation** remains an elusive but important future.  \n  - Likely to remain a system/compiler concern rather than widely exposed to programmers.  \n  - Parallelism should not be prematurely optimized.\n\n- There will be **many programming languages**, especially domain-specific ones layered over a few general-purpose languages.\n\n- **Language design increasingly driven by open-source hackers** and application programmers over academia.\n\n- Designing future languages should start from **ideal programs, ignoring current language biases**, focusing on minimal parse tree size for ease of writing.\n\n- The language of the distant future can be designed today and may even be beneficial to use now.\n\n---\n\n## Important Quotes\n\n- \"Staying close to the main branches is a useful heuristic for finding languages good to program in now.\"\n- \"Cruft breeds more cruft; smaller, cleaner cores are more likely to survive evolution.\"\n- \"Programmer time is more valuable than machine time, especially as computers get faster.\"\n- \"Parallelism is often wasted rather than used well.\"\n\n---\n\n## Action Items / Recommendations\n\n- Aim language design toward minimal axioms and elegant cores to ensure longevity and adaptability.\n- Embrace multi-layered software architecture for flexibility despite its speed costs.\n- Favor programmer productivity over machine efficiency when possible, leveraging increasing hardware speeds.\n- Avoid premature optimization, particularly with parallelism.\n- Encourage open-source and application programmer involvement in language design.\n- Keep a long-term vision (\"distant target\") in language evolution to guide current development.\n\n---\n\n## Additional Notes\n\n- Lisp Machine Lisp and Common Lisp introduced the idea that declarations are optimization hints, not altering program meaning.\n- The essay credits contributions from prominent hackers and the Python community.\n\n```",
  "category": "programming",
  "save_result": "saved with ID 492b157e",
  "timing": {
    "initialize_time": 0.0004150867462158203,
    "format_time": 15.988846063613892,
    "summarize_time": 12.720906019210815,
    "categorize_time": 4.0156331062316895,
    "save_time": 0.0014929771423339844
  },
  "errors": [],
  "updated_at": "2025-05-30T16:28:10.859098",
  "content_length": 55752,
  "content_words": 9608,
  "estimated_tokens": 13938,
  "formatted_length": 7474,
  "format_result": "formatted successfully",
  "summary_length": 3770,
  "summarize_result": "summarized successfully",
  "category_properties": {
    "title": "What Programming Will Be Like in a Hundred Years",
    "author": "Paul Graham",
    "date": "April 2003",
    "type": "essay",
    "source_event": "PyCon 2003 keynote talk",
    "main_topics": [
      "future of programming languages",
      "programming language evolution",
      "software development trends"
    ],
    "key_points": [
      "Difficult to predict exact future but some technological assumptions made",
      "Programming languages evolve like species with evolutionary dead ends",
      "Examples of dead-end languages such as COBOL and predicted similar fate for Java"
    ]
  },
  "categorize_result": "categorized as programming",
  "memory_id": "492b157e",
  "final_title": "\n    yp this essay is form paul grahm can u save i...",
  "total_time": 32.733662128448486,
  "completed_at": "2025-05-30T16:28:10.859092"
}